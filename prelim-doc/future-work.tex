\chapter{Ongoing and Future Work: Translator-agnostic Translation
  Validation}\label{sec:future}

With the overall goal of  establishing faithfulness of binary lifters, we
propose to use \tv to validate the translation from binary to LLVM IR\footnote{However, we believe that the core techniques are
    applicable to other decompilers targeting mid-level (language-neutral) IRs}. As
mentioned earlier, we employed \K framework as the formalism medium to
represent the source and target language and in the process, defined the most
complete semantics of user-level \ISA using the framework. We proposed to
borrow a language-independent equivalence checking algorithm
\m{Keq}~\cite{TheoSAS19} that can be parameterized with the input and output
language semantics. 
%The algorithm needs, as input, the \syncps for establishing equivalence
%between binary and LLVM IR programs, which can be generated by the translator
%itself.  .

As mentioned in Section~\ref{sec:approach}, the notion of equivalence between
binary program and the lifted LLVM IR is formalized as a bi-simulation
relation~\cite{Sangiorgi:2011} between pairs of states of the two programs.
These program points, also referred to as \syncps, are generated by a proof generator and fed to the equivalence
checker as inputs. There are various ways a proof generator can generate these
\syncps each having different challenges and trade-offs.

\section{Translator-specific approaches to SYNCHRONIZATION POINT
  generation}\label{sec:TS} One approach to generate the \syncps is by
  instrumenting the translator itself. 
%
% and can
%be generated, by a proof generator,  as hints by analyzing the translator
%itself. 
The translator, being aware of which high-level IR instructions and control
points are emitted for each binary instruction, can accurately generate the
\syncps. Moreover, each \syncp is labeled with invariants over symbolic
variables, corresponding to input/output program states, in order to cover only
observably equivalent states. For each \syncp, the equivalence-checker checks
equivalence by delegating proof obligations to an SMT solver. Failure of a
proof obligation can be attributed to either a bug in the translator or  the
fact that the source and target programs are not equivalent in the first place.
We can rule out the second possibility because the source and target programs
are not selected arbitrarily but they are the actual input and output of a
translator, under test, and hence claimed to be equivalent by the translator
itself.

However, the above approach requires modifications of the translator to
generate the \syncp points and hence it may not be easily scalable to other
decompilers. 

Another approach is to analyze the source (\ISA) code and target (LLVM IR) code
of a particular translation  and infer the synchronization points.
We note that the analyzer has to take into account translators specific idioms
while analyzing the target code because different binary translators might use
different ways to lift the binary code in the target IR. We would demonstrate
the applicability of this approach by validating the translation of a realistic
decompiler like McSema~\cite{McSema:Recon14}

\section{Translator-agnostic approaches to SYNCHRONIZATION POINT
  generation}\label{sec:TA}

The fact that the proof generator is using translator generated hints or idioms used by the translator
to  generate the \syncps \& the associated  invariants,  makes it  translator specific and hence not applicable
uniformly across translators. We aim for a  binary-translator-agnostic proof
generator, which, indeed, makes the problem  even more challenging. We would
like to explore the following  approaches towards that goal.

\paragraph{\textbf{Data-driven \syncp generation}} This approach
is based on inferring the \syncps using test-runs. There are two
notables~\cite{Iman2005,DDEC:OOPSLA:2013} along this direction which
deals with checking equivalence between GCC~\cite{GCC} RTLs or x86 loops respectively.

Iman \etal~\cite{Iman2005} proposed an algorithm for solving the problem of
finding basic block and variable correspondence between two GCC RTL programs
compiled from the same source, but using different optimizations.  The essence
of their technique is interpretation of the two programs on random inputs and
comparing the histories of value changes for variables.  If the sequences of
value changes of two variables are the same, then the variables probably
correspond to each other, and the block in which the changes occur might also
correspond to each other. However, the paper candidly admits that they do not consider  generation of
verification invariants (VCs) at a point of correspondence or proving the VCs, hence it is difficult
to judge  its effectiveness in proving the equivalence of the candidate RTL
programs.  \cmt{ Moreover, the paper do not try to address any reordering
  transformation, that is any transformation that changes the order of
    execution of code, without adding or deleting any executions of any
    statement. Also, using random values results in poor code coverage which is
    detrimental to finding variable correspondence. We note that only the basic
    blocks \m{b1} and \m{b2}, which are covered by the same test input run in
    original and optimized versions respectively, can be the candidate for
    correspondence.}   


%DDEC~\cite{DDEC:OOPSLA:2013} uses a combination of static analysis and
%data-driven inference for constructing simulation relations: Static analysis is
%used to determine the program locations of \syncps and the live
%variables while the constraints between variables are inferred from execution
%traces. 


Rahul \etal~\cite{DDEC:OOPSLA:2013} presented ``data-driven equivalence
checker'' (DDEC) to check equivalence of two x86 loops which uses data-driven
inference in order to guess candidate simulation relations\footnote{A 
  simulation relation can be roughly defined to consist of a pair of program cutpoints, which breaks two
    loops into a set of pairs of loop-free code fragments and  linear
    equalities as invariants to hold at those cutpoints.}. Their approach does
    not assume any knowledge about the optimization performed while determining
    those simulation relations. The candidate simulation relations are checked
    using an SMT solver to determine equivalence of the loops. The cutpoints
    are chosen where the corresponding memory states agree on the largest
    number of values. The equality conditions   at cutpoint is determined by
    using standard linear algebra techniques over the values of live variables
    recorded by inserting instrumentation. Moreover, DDEC is sound; a lack of
    test coverage may cause equivalence checking to fail, but it cannot result
    in the unsound conclusion that two loops are equivalent when they are not.
    \cmt{However, while DDEC is trying to prove that two x86 loops are
      equivalent, the proof obligation might fail because  of one or more of
        the following reasons: (1 ) If satisfactory cutpoints are not found,
            (2) If spurious equality relationships are produced as invariants,
            (3) There is a bug in the translator, and (4) The target and the
              rewrite are not equivalent in the first place. The attribution of
              failure reason seems non-trivial and could affect fully automatic
              application of the approach.}   

%Our proposed approach is based on the insight that a sound guess of
%synchronization points do not result in proving two programs equivalent when
%they are not (false positives). Indeed, they might produce false negatives.
%Having said that, 

Our proposed approach is based on the insight that
given  \ISA and translated LLVM programs, we might compile
the LLVM program to binary and use the data-driven
approach~\cite{DDEC:OOPSLA:2013}, mentioned above, to extract guesses of
\syncps. Next, we can map the \syncps back to the IR level and employ the equivalence
checker \m{Keq} to prove the equivalence of the resulting LLVM IR and the
original binary using the \syncp guesses. We note that, an incorrectly guessed, but sound \syncp will never result in proving two programs equivalent when
they are not (false positives) and hence we do not have to trust the compilation back-end.

%However, we do not want to add the compilation in our trust base and
%hence we can map the \syncps back to the IR level and employ the equivalence
%checker \m{Keq} to prove the equivalence of the resulting LLVM IR and the
%original binary using the \syncp guesses. 
However, these \syncp, being
data-driven guesses, may lead to failing the proof obligations for reasons
other than a translation error. For example, the proof might fail if the
spurious invariants are generated or the \syncps are not sufficient, and it is
highly non-trivial, but an interesting research problem, to automatically
attribute the failure reasons. 

There is a trade-off between translator-specific and translator-agnostic proof
generator approaches. The \syncps generated by the former is accurate, leading
to trivial and automatic attribution of proof failure reason. On the other
hand, the \syncp guesses produced by the  translator-agnostic proof generator
has issues with automatic attribution  of the failure reasons. We envision
opportunities to exploit this trade-off by pruning off \syncp  guesses which a
translator specific proof generator will never generate. For
  example, the equivalence checker \m{Keq}~\cite{TheoSAS19} is used for
    validating the compilation pipeline and it uses compiler generated hints to
    prove equivalence. The underlying algorithm mandates three
%equivalence-checker \m{Keq}~\cite{TheoSAS19}, which employs a
%translator-specific proof generator  to validate compilation,  dictates three
broad categories of synchronization points which are necessary and sufficient
to prove equivalence, viz., entry points, exiting points (which also include
    points before callsites), and the rest of the points (including loop entry
      points and points after callsites). We may use this information to prune
    redundant \syncp guesses generated by a translator-agnostic  proof
    generator like DDEC~\cite{DDEC:OOPSLA:2013}.

\paragraph{\textbf{Learning \syncps}}

As discussed in Section~\ref{sec:ml}, there exist many recent
efforts~\cite{Jaffe:2018ICPC,Rosenblum2007,Rosenblum:2008,Rosenblum:2010,Rosenblum:2011,Bao:2014,Shin:2015}
in extracting various features (like meaningful variable names, compiler
    provenance, function entry points) using machine learning techniques which
are useful for binary analysis. Most notable is the novel technique presented
by Katz \etal~\cite{katz2018rnn}  for decompiling binary code snippets using a
model based on Recurrent Neural Networks. The model learns properties and
patterns that occur in source code and uses them to produce decompilation
output. 
%

The employed model is  a sequence-to-sequence model based on RNNs. The model
composed of two recurrent neural networks: an encoder, which processes the
input binary sequence, and a decoder, which creates outputs, the decompiled C
code. Further, the sequence-to-sequence model contains a hidden state, which
summarizes the entire input sequence. 
%
The training input contains of a large pair of strings representing the snippet
of source C code and the corresponding binary output, derived from open source
projects.  The snippets are preprocessed  by tokenizing each string  and
translating the resulting token list to a list of integers. Preprocessing also
produces a dictionary to map which integers correspond to which tokens in both
the higher-level and binary snippets.  The encoder layer of the RNN is trained
using the list of integers, derived from binary and the decoder layer is
trained using the corresponding list of integers, derived from C code.  During
training, the encoder-decoder modifies its internal state and does not provide
additional output. For testing, the test-binary snippets are translated into
lists of integers, which is then fed to the trained encoder-decoder model to
output a list of integers, which  is later turned into  a predicted higher-level C
translation using the dictionary  generated in the preprocessing step.
%
There evaluation results suggest that the technique  can be used to obtain a high level
syntactic structure of the code.
    
    
For an input binary and the high-level IR, translated by an arbitrary
decompiler, our goal is to learn the corresponding \syncp. Using a
straightforward  supervised-learning approach, we could frame the problem as:
To predict if an arbitrary pair of program points is a \syncp or not?
Such a supervised-learning formulation requires providing  many input code
snippets, correctly labeled with \syncps, as the training set in the first
place. Indeed, it is interesting to explore how to harvest the input corpus. 
 

Another way of inferring the \syncps is by using ``Trial and Error'', for which
Reinforcement Learning~\cite{Sutton:1998} seems to be a better choice to
explore. For the ``Trial and Error'' approach to work, there must be an
efficient way to find the error state, namely, the tried \syncp is non-valid.
With that, the goal of inferring all the corresponding \syncps of an input
output program pair can be defined solely in terms of some \emph{reward} given
on each selection. For example, for each selection of \syncp, a \emph{reward}
can be assigned using the number of values on which the corresponding memory
states agree on, when they are both tested using a random set of same  inputs.

We believe such learning techniques are promising for this problem and we would like to explore more on this in the proposed work.
