\chapter{Related Work}\label{sec:related-work}

Given the importance of establishing the faithfulness of the binary lifters,
      there exists a couple of efforts towards that direction, which we will elaborate on next.

\section{Recent Advances in Validation of Decompilation}\label{sec:recent-advances}
All the previous approaches can be broadly categorized to be based on (1)
  Simulation-testing, (2) Formal Methods, or (3) Machine Learning.  

\subsection{Approaches using Simulation-Testing}
This approach is similar to black-box testing in software engineering. Most
notable work include Martignoni et
al.~\cite{Martignoni:ISSTA2009, Martignoni:ISSTA2010,Martignoni:ASPLOS2012} and
Chen \etal~\cite{CLSS2015}.


Martignoni et al.~\cite{Martignoni:ISSTA2009, Martignoni:ISSTA2010} proposes
hardware-cosimulation based testing on QEMU~\cite{QEMU:USENIX05} and
Bochs~\cite{Bochs1996}.  Specifically, they compared the state between actual
CPU and  IA-32 CPU emulator (under test) after executing randomly selected
test-inputs on randomly chosen instructions  to discover any semantic
deviations.
%Specifically, they compared the state between a physical and an emulated CPU
%after executing randomly chosen instructions on both to discover any semantic
%deviations. 
Although, a simple and scalable approach, it's effectiveness is limited because
many semantics bugs in binary lifters are triggered upon a specific input and
exercising all such corner inputs, using randomly generated test-cases, is
impractical.
%%

Chen \etal~\cite{CLSS2015} proposed validating the static binary translator
LLBT~\cite{LLBT2012} and the hybrid binary translator~\cite{LLVMDBT2012},
  translating ARM programs to x86 programs using the LLVM x86 backend. First, an
  ARM program is translated offline to x86 program. Next, the translated x86
  binary is executed  directly on a x86 system while the original ARM binary runs on the QEMU emulator. During run time, both the ARM binary and the
  translated x86 binary produce a sequence of  architecture states, which are
  compared at the granularity of single instruction or single basic block. They evaluate their validator using the ARM code compiled from
  EEMBC 1.1 benchmark. 
%%

Martignoni \etal~\cite{Martignoni:ASPLOS2012} uses symbolic execution on a
Hi-Fi emulator~\cite{Bochs1996}, defined  as a binary emulator which is more
complete in terms of instructions coverage of IA-32 ISA and faithful, to
generate high-quality test cases to validate  a Lo-Fi
emulator~\cite{QEMU:USENIX05}, defined as less complete and buggier emulator.
The validation works as follows: A randomly chosen binary instruction is
executed twice, once on a real hardware and next on the Lo-Fi emulator, and the
output states are matched.
%
Note that, even though Martignoni
\etal~\cite{Martignoni:ASPLOS2012} symbolically explored the test-cases which
is supposed to cover all the paths of a given instruction's implementation, but
being a differential testing-based approach, the faithfulness depends directly
on  the faithfulness of the Hi-Fi emulator. A wrong implementation (or even
    omission of a particular case) of instruction semantics in the Hi-Fi, will
lead to test-cases insufficient to explore all the paths and hence find bugs in
the Low-Fi emulator\footnote{We note that our proposed semantics-driven \tv approach shares similar assumptions about the faithfulness of the semantics.}. 
%
Moreover, the symbolic execution of an instruction's implementation in the
Hi-Fi emulator is achieved using an X86 interpreter FuzzBALL. A bug in the
interpreter will affect the generation of high-fidelity test cases for a particular
instruction, leading to incomplete coverage of that instruction's implementation
in Low-Fi emulator.
%
\cmt{However, their approach does not consider the floating point instruction
  because the employed symbolic execution engine (FuzzBALL) does not support
    it.} Also, the method can capture  deviations in the behavior of only those
    instructions which are implemented in both the emulators.

%    Schwartz \etal~\cite{Schwartz:2013} proposed control flow structure recovery by
%    employing semantics preventing schema and tested their binary to C decompiler,
%    Phoenix, which is based on BAP~\cite{BAP:CAV11}, on a set of 107 real
%    world programs from GNU coreutils. Along similar lines, 
%    %
%    Yakdan \etal~\cite{Yakdan2015NDSS} presented a decompiler, DREAM, to offer a
%    goto-free output. DREAM uses a novel pattern independent control-flow
%    structuring algorithm that can recover all control constructs in binary
%    programs and produce structured decompiled code without any goto statement. The
%    correctness of our algorithms is demonstrated using the GNU coreutils suite of
%    utilities as a benchmark.
%    
%    Andriesse \etal~\cite{nucleus2017EuroSP} proposes a function detection
%    algorithm, Nucleus, for binaries. The algorithm does not require function
%    signature information or any learning phase. They evaluated Nucleus on a
%    diverse set of $476$ C and C++ binaries, compiled with gcc, clang and Visual
%    Studio for x86 and x64, at optimization levels O0--O3. 
%    
%    Martignoni et al.~\cite{Martignoni:ISSTA2009, Martignoni:ISSTA2010} attempted
%    to leverage differential testing on QEMU~\cite{QEMU:USENIX05} and
%    Bochs~\cite{Bochs1996}. Particularly, they compared the state between a
%    physical and an emulated CPU after executing randomly chosen instructions on
%    both to discover any semantic deviations. Although their technique can be
%    applied to testing binary lifters, it is fundamentally limited because its
%    effectiveness largely depends on randomly generated test cases. Typically,
%    semantic bugs in binary lifters are triggered only with specific
%    operand values. Therefore, a random test case generation does not
%    help much in finding such bugs.

\subsection{Using Formal Methods}
Another direction to establish strong guarantees in the binary translation is by using
formal methods. The efforts along the direction include Soomin
\etal~\cite{ASE2017}, Myreen et al.~\cite{Myreen:FMCAD:2008,Myreen:FMCAD:2012}
and Fr\c{e}d\c{e}ric \etal~\cite{inlineassm}.
%%

MeanDiff~\cite{ASE2017} proposed an N-version IR testing to validate three binary
lifters, BAP~\cite{BAP:CAV11}, BINSEC~\cite{BINSEC2011}, and PyVEX~\cite{PYVEX}
by comparing their translation of a single binary instruction to BIL, DBA, and VEX IRs respectively.
The tools symbolically execute each of the IR instances, lifted from a single
binary instruction, to generate symbolic summaries to be compared using a SAT
solver. MeanDiff neither handle floating point operations, nor the instructions
which does not manifest their side-effects (like flag updates) explicitly.
Moreover, MeanDiff reports a bug whenever a deviation is detected w.r.t the
instruction-semantics-behavior in at least two binary lifters. But even if all
the binary lifters are in sync on the behavior of a particular instruction, we
cannot guarantee that all the lifters are faithful in lifting that instruction,
       which is however, a general limitation of differential testing based
       approach. Also, as MeanDiff is testing multiple binary lifters
       together, hence it cannot be used to establish the faithfulness in
       lifting the semantics of an instruction which is not implemented in any one
       of them.
       %
       \cmt{ 
       Leveraging symbolic summaries of source and target programs to prove the
       correctness of  compilation/optimization is a well-researched topic. In the context of
       binary translation, MeanDiff~\cite{ASE2017} has demonstrated how
       symbolic summaries can help in N-version equivalence testing of binary
       lifters. However, they compared the summaries corresponding to a single
       instruction at a time.}
%%

Myreen et al.~\cite{Myreen:FMCAD:2008,Myreen:FMCAD:2012} proposed
``decompilation into logic'' which, given some concrete machine code and a
model of an ISA, extracts logic functions or symbolic summaries which captures
the functional behavior of the machine code. The decompiler works on top of ISA
models for IA-32 \cite{Karl2003}, ARM~\cite{Fox2003} and
PowerPC~\cite{Leroy:2006}. Assuming that the ISA models are trusted, the
extracted functions can be used to prove properties of the original machine
code. However, the work has not been applied to validate the binary translation to an IR.
A recent work by Roessle et al.~\cite{Roessle:CPP19} improves the
aforementioned idea  by including a subset of \ISA, derived mostly from
Strata~\cite{Heule2016a}, in the trust-base of ISA models.   



\cmt{Myreen et al.~\cite{Myreen:FMCAD:2008,Myreen:FMCAD:2012}
extracted function-level symbolic summaries which indeed is a promising
building block towards establishing correctness of binary lifters.\cmt{, which, however,  has many additional challenges to deal with (Refer
             Section~\ref{sec:challenges}). Moreover,} However, both Myreen et al. and
         Roessle et al. have limited \ISA instruction set coverage, which
         might restricts their application on many  real-world binaries.}


\subsection{Using Machine Learning} Another recent work by Schulte
\etal~\cite{eschulte2018bed} proposed Byte-Equivalent Decompilation (BED) which
leveraged a genetic optimization algorithm to infer C source code from a
binary. Given a target binary and an initial population of C code as
decompilation candidates, they  drive a genetic algorithm to improve the
initial candidates, driving them closer (using compilation to binary) to
byte-equivalence w.r.t the target binary. The byte equivalence  is simply the
edit distance to the target binary. As hypothesized in the future work section
of the paper~\cite{eschulte2018bed}, BED could be applied to LLVM IR instead of
C to evolve lifting from machine code to LLVM IR and may work well due to the
relative simplicity of LLVM IR as compared to the C. Being byte-equivalent, the
generated LLVM IR will be the faithful evolution from the machine code.
However, as shown in the paper, this approach worked moderately well for
smallish binaries. For example, out of $22$ binaries under test, only $4$ achieve full byte equivalence when the initial
    population is augmented with decompilation candidates from the
    HEX-RAYS~\cite{hexray} Decompiler. It is still an open problem to realized
    an end-end byte-equivalent binary to LLVM decompiler using purely genetic
    optimization algorithm. 
\cmt{only $3$
    achieve full byte equivalence when the initial population does not include
    decompiler seeds, and} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Next, we elaborate on the state-of-the-art on various techniques and concepts
which we propose to borrow as the basic ingredients to build our approach on. Those
include Equivalence checking, \TV, and Software Verification, and Machine Learning-Assisted Binary Code Analysis.

\section{Translation Validation}

Pnueli \etal~\cite{Pnueli:1998} proposed the idea of \tv as a new approach to
the verification of translators (compilers, code generators). The idea is:
Rather than verifying the compiler itself, one constructs a validation tool
which, after every run of the compiler, formally confirms that the target code
produced in the run is a correct translation of the source program. One of the
important ingredients  to set up the  translation validation process involve a
formalization of the notion of ``correct implementation'' as a refinement
relation. As proof method for the refinement, they employ a generalization of
the well-established concept of simulation with refinement
mapping~\cite{Abadi:1991}. Refinement mappings define a correspondence between
the variables of a concrete system and the variables of an abstract system such
that observations are preserved. 

Hawblitzel et al.~\cite{Hawblitzel:FSE2013} use a \tv approach to determine
whether assembly code produced by different versions of the CLR JIT compiler
are semantically equivalent and thus report mis-compilations when there are
differences. The versions include those across a seven-month time period,
  across two architectures (x86 and ARM), and across optimizations levels. The
  underlying validator encodes each assembly method body into a procedure in
  the Boogie~\cite{Boogie:2005}  programming language and then invokes the
  SymDiff symbolic differencing tool~\cite{SYMDIFF:2012} to compare the Boogie
  encodings for semantic equivalence. For code with loops, the validator simply
  eliminates loops by unrolling them n (= 2) times, ignoring any behaviors past
  the n$^{th}$ iteration.
 


Translation validation  has been employed heavily in the field of compiler
correctness~\cite{VOC2002,TVOC:CAV2005,Necula:2000}.  Necula~\cite{Necula:2000}
proposed a technique where each of the original and the optimized programs is
firstly evaluated symbolically into a series of mutually recursive function
definitions. A basic block and variable correspondence is inferred by a
scanning algorithm that traverses the function definitions. The algorithm
generates both a relation between program points and the accompanying
constraints between program variables and memory at the program point.  

For example, when the scanning
algorithm visits a branch condition \m{e} in the original program, it
determines whether \m{e} is eliminated due to the optimizations. If it is
eliminated, then the information collected is either \m{e = 0} or \m{$\sim$e =
  0}, depending on which branch of \m{e} is preserved in the optimized program. 
%
If \m{e} is not eliminated, then it corresponds to another branch condition
\m{e'} in the optimized program. The information collected is either \m{e = e'}
or \m{e = $\sim$e'}, depending on the correspondence of \m{e}’s and \m{e'}’s
branches. One of the limitations of the algorithm is that all branches in the
target program must correspond to branches in the source program.  \cmt{ This
  shows that, besides symbolic evaluation, Necula’s technique has to solve some
    equalities to determine which branches are eliminated and also to determine
    the correspondence between branches in the two programs.} Moreover, to find
    a basic block correspondence Necula’s technique uses some heuristics which
    are specific to the GNU C compiler. 
  
\cmt{ 
Another translation-validation technique is VOC [11]. We overview VOC for struc-
ture preserving transformations only. Such transformations admit a mapping
between some program points in P and P'. In VOC a basic block and variable
correspondence is represented by a mapping from some blocks in P' to some
blocks in P, and also by a data abstraction. The domain and range of the block
mapping form sets of control blocks. VOC chooses the first block of each loop
body as a control block. The data abstraction is constructed as follows. For
each block Bi in P', and for every path from block Bj leading to Bi, a set of
equalities v = V is computed, where v and V are vari- ables in P and P'
respectively. The equalities are implied by invariants reaching Bj, transition
system representing the path fromBj to Bi and its counterpart in P,and the
current constructed data abstraction. This requires the implementation of VOC
to use a prover to generate a data abstraction. Moreover, an implementation of
VOC for Intel’s ORC compiler, VOC-64, tries the variable equalities for every
pair of variables except for the temporaries introduced by the compiler. This
trial is performed by scanning the symbol table produced by the compiler [2].
However, not every compiler provides the symbol table as a result of
compilation, thus this limits the applicability of VOC-64.}
  
The translation validation technique by Rival~\cite{Rival:2004} provides a unifying framework for
the certification of compilation and of compiled programs. Similarly to
Necula’s technique, the framework is based on a symbolic representation of the
semantics of the programs. Rival’s technique extracts basic block and variable
correspondence from the standard debugging information if no optimizations are
applied. However, when some optimizations are involved in the compilation, the
optimizing phase has to be instrumented further to debug the optimized code and
generate the correspondence between the original and the optimized programs.
One technique to automatically generate such a correspondence is due to
Jaramillo et. al [4].  In this technique, the optimized programs initially
starts as an identical copy of the original one, so that the mapping starts as
an identity. As each transformation is applied, the mapping is changed to
reflect the effects of the transformation. Thus, in this technique, one needs
to know what and in which order the transformations are applied by the
optimizing phase.  

\cmt{ 
DDEC~\cite{DDEC:OOPSLA:2013} is a data-driven equivalence checker for x86
loops that uses data collected from test runs rather than inference or hints to
construct a simulation relation. For evaluation, they prove the equivalence of
code produced by \textsc{COMPCERT}~\cite{CompCert:FM06} and gcc (with
    optimization enabled).
}
%DDEC~\cite{DDEC:OOPSLA:2013} uses a combination of static analysis and
%data-driven inference for constructing simulation relations: Static analysis is
%used to determine the program locations of \syncps and the live
%variables while the constraints between variables are inferred from execution
%traces. 



\section{Machine Learning-Assisted Binary Code Analysis}\label{sec:ml}

Hasabnis \etal~\cite{Hasabnis:ASPLOS16, Hasabnis:FSE16} presents the semantics of \ISA using machine learning~\cite{Hasabnis:ASPLOS16} and symbolic 
execution~\cite{Hasabnis:FSE16} to automatically learn the translation of \ISA instructions to their IR, by extracting knowledge from the hard-coded  translation logic of compilers such as GCC.
However, as they admitted~\cite{Hasabnis:FSE16}, their semantics omits some important details of \ISA semantics (e.g., the effect of various instructions on CPU flags).


Jaffe \etal~\cite{Jaffe:2018ICPC} proposes a technique to assign meaningful
variable names to Hex-ray~\cite{hexray} decompiled C code by learning names
that developers have assigned to code used in similar contexts. The technique
aligns the variables in the decompiler output with those in the original C code
in order to generate an aligned parallel corpus that is suitable for training a
Statistical Machine Translation model~\cite{Koehn:2007}.  

Rosenblum \etal~\cite{Rosenblum2007,Rosenblum:2008}%,Bao:2014,Shin:2015 
consider
the machine learning problem of identifying function entry points in binaries
where symbols indicating function location are stripped.
%
Rosenblum \etal~\cite{Rosenblum:2010} formulate compiler identification as a
structured learning task, automatically building models that classify sequences
of stripped binary code by the generating compiler. They train their model
using binaries compiled using GCC and ICC and labeling each binary program
point with a particular compiler.
%

Rosenblum \etal~\cite{Rosenblum:2011} developed an authorship-attribution
technique that uses machine learning approach to automatically discover the
stylistic characteristics of binary code which are indicative of programmer
style.


%% Using Formal Methods
\cmt{ 
    , and it is non-trivial to achieve the same equivalence using function level summaries, 
    which is less harder problem that proving equivalence between function level summaries.   
    
    Many \TV system makes use of symbolic summaries of source and target programs to validate the translation.
    Leveraging symbolic summaries of machine code are important building blocks for validating its translation to LLVM IR, 
    proof-producing decompilation where they translates a sequence of machine code
    to a tail-recursive functions defined in the language of a theorem prover,
    which accurately describes the effect of the given machine code and hides
    irrelevant details of the underlying machine language specification. Along
    similar lines,  


    Fr\c{e}d\c{e}ric \etal~\cite{inlineassm} addresses the challenge of designing and developing an
    automated, generic, trustable, and verification-oriented lifting technique
    turning inline assembly into semantically equivalent C code. By focusing on
    inline assembly rather than arbitrary decompilation, they tackle a problem both
    more restricted (simple control-flow, smaller size) and better defined
    (interfaces with C code, no dynamic jumps). Their idea goes by: (1) Compiling
    the source C code containing inline assembly to binary with debug information,
    where the assembly code is propagated as is, (2) lifting the output of 1 to
    an DBA IR using BINSEC~\cite{BINSEC2011}, (3) Lifting the assembly
    counterpart of the lifted IR at 2 to C, thereby augmenting the  source C
    code, (4) Recompiling the augmented C code to IR, and (5) translation
    validation of IRs at 2 and 4. This means that their work in basically
    validating the lifting of assembly code to C, not the translation of the
    binary to DBA IR, which is included in the trusted computing base. That
    lifting takes care of type reconstruction, register unpacking, structuring,
    predicate recovery. The float point operations are skipped because of the
    lack of support in BINSEC.   
    %%
}
