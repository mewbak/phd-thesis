\chapter{Introduction}\label{sec:ba}

%The problem you want to tackle
Analysing binary code is crucial in software engineering and security research.
Some of the notable applications of binary analysis can be found in binary
instrumentation [13], [36], [40], [46], binary translation [20], software
hardening [16], [26], [59], [60], software testing [9], [17], [27], CPU
emulation [12], [42], malware detection [18], [33], automated reverse
engineering [21], [38], [39], [52], [57], and automatic exploit generation
[15]. Binary analysis is generally performed by existing decompiler projects
~\cite{McSema:Recon14,Remill,Angr1,BAP:CAV11,Radare2}, whose very first step is
to translate se machine code to a intermediate representation (IR), and thereby
exposing many high-level properties (like control flow, function boundary and
    prototype, variable and their type etc.) of the the binary, which  assist
further analysis and/or optimization. Formally establishing faithfulness of the
decompilation (i.e. translation from machine code to high level IR) is pivotal
to gain trust in any binary analysis. Any bug in the translation would
%What is the current state of the art and why that is insufficient
invalidate the binary analysis results. Despite of the importance in
establishing the faithfulness of the binary lifters, there has been
surprisingly little effort towards that direction. The most widely used
approaches in validating the binary lifters are either based on
hardware-cosimulation testing~\cite{Martignoni:ISSTA2009,Martignoni:ISSTA2010},
  which is limited by the fact that generating specific test-cases to uncover
  semantic bugs in the lifter is non-trivial, or differential
  testing~\cite{Martignoni:ASPLOS2012,ASE2017}, which is limited in terms of
  instruction coverage and faithful-ness guarentess (Refer to
      Chapter~\ref{related-work}).

%How you plan to improve on the state of the art
The goal of our proposal is employ \TV on binary lifters by leveraging
the semantics of the languages involved (e.g. the Intel's X86-64 and the
    high-level IR).  Given the recent advances in translation
validation~\cite{} in validating compilation~\cite{}, a basic version of this
strategy is likely quite feasible today, however, there are additional
challenges to deal with when it comes to validating the decompilation
pipeline(Refer Chapter~\ref{approach}). We would like validation approach to be
general enough and hence applicable to any state-of-the-art binary to LLVM IR
translators~\cite{McSema:Recon14,Remill,FCD,reopt,llvm-mctoll}, thereby
avoiding any translator specific customization during validation.  However,
         having this goal makes the problem even more challenging.

We believe that formally validating the translation is more robust as
compared to (1) validating the translation using random (or low coverage)
  test-cases, and (2) differential testing technique which proves the
  correctness of a translation, provided by a translator, by comparing its behaviors with
  that provided by other translators under test. This means,
  declaring a translation to be correct assumes the correctness of all the
  translator.

%Summarize your contributions
\paragraph{Contributions}
Below are our primary contributions.

\emph{Employing \TV on binary translators~} We propose tools and techniques to
formally validate the translation from binary to high level IR. To the best of
our knowledge, we are the first to propose usig \TV to establish the
faithfulness of binary lifters targetting LLVM as their high-level IR.

\emph{Translator-agnostic solution~} We would like our validator to work
uniformly across translators without using any translator-generated hints or
translator-specific heuristics.


In the ext section, we provide the motivation behind the importance binary analysis.
%Why it is important
\section{Benefits of Binary Analysis}
Analysis and reasoning about source code is one of the most pervasive concepts
in computer science research. Analyzing the code to approximate  the semantics
of the program helps in determining the correctness of the program as per some
gold standard or determining illegal memory and control flow accesses, or
proving/refuting various properties of interest to the users. Static
analysis~\cite{Nielson2010}, model checking~\cite{Clarke1981,Queille1982}, and
abstract interpretation~\cite{Cousot1977} are the well known techniques used,
         widely and with ease, for the  analysis of source code and have been
         deployed in many tools and processes that improve the software
         quality~\cite{Xie:2003,Musuvathi:2008,Ivancic:2005,Dwyer:2007,Binkley:2007,Bessey2010,Ball2006}.
         Once the software, written in some high-level language, is compiled
         into binary format and shipped, the users down the line have to trust
         the vendor and the distributors about the quality and security of the
         product. Even when the distributors can be trusted, the compilation
         pipeline might introduce a bug in the binary and hence ensuring trust
         in the binary requires the compiler to be in the trusted computing
         base.  All the static source code analysis techniques, mentioned
         above, are targeted to human readable source code written in high
         level languages as apposed to low level binary code. Analysis at the
         binary level is difficult mainly because many source level
         information, e.g. loops, procedures, or classes which provides a
         natural structural partitioning of programs into functionally related
         units and assist source level analysis, are completely or partially
         lost during the compilation process. Moreover, some source level
         information, like symbol information, types, function boundaries and
         their prototypes, which creates a logical view of the program  within
         a structural partition, is also stripped off during the compilation
         process. Absence of symbol information and types means that variables
         are not easily identified, but are represented by reusable registers
         and the memory, which is addressable as a large continuous array.
         Registers and memory carry no type information, and pointers of any
         type are indistinguishable from integers.  Despite of these
         difficulties, there are several compelling reasons to do analysis at
         the binary level, We enumerate the most important ones as follows:
\begin{itemize}

    \item Analyzing stripped binary executables, i.e., binaries without symbol
    or debugging information, enables software analysis without access to
    source code. Such scenarioes arises in the case of (1) legacy code, when
    binary analysis is the only viable option to re-implement (or patch) the
    program, or (2) malwares, when binary analysis helps in security audits or
    malware
    detection~\cite{Christodorescu:2005,Andreas2007,Kinder:2005,Kinder:2010,Kolbitsch:2009}.
    
    \item There are challenges that the source code based analysis tools has to
    face  in dealing with the code written in different feature-rich high-level
    languages, for example, parsing support for  the high-level constructs.
    Moreover, while analyzing source code, the libraries are often replaced by
    coarse grained abstractions~\cite{libabs}. Operating on the binary avoids
    these issues altogether, since all source languages are translated into a
    hardware specific, but single target language with no distinction between
    the source code or library code. However, a common workaround for these
    problems, which already in common practice, is to preprocess input files
    into a simpler intermediate form which is amenable for analysis. For
    example, for languages that are compiled to an intermediate form, such as
    Java bytecode, Microsoft's Common Intermediate Language (CIL), or
    LLVM~\cite{Lattner:2004} IR, it is already common practice to analyze IR
    instead of source, in order to avoid problems from parsing and to support
    all the different source language idiosyncrasies.
    
    \item Even when the source code is available, doing analysis at the binary
    level alleviates the need to trust the correctness of the compiler.
    Moreover, during the compilation process, the source code undergoes many
    modification, removal or additions, before translated to binary and
    analyzing that binary is desirable because it is what is executed on
    hardware. 
    
    \item Binary analysis is heavily leveraged by various tools, ranging from
    software emulation and
    virtualization~\cite{QEMU:USENIX05,Valgrind:ENTCS03,DynamoRIO:2004,Pin:2005},
    malware analysis~\cite{BitBlaze:2008,BAP:CAV11,Egele:USENIX07,Yin:CCS07},
    reverse engineering~\cite{McSema:Recon14,Angr,Radare2},
    sand-boxing~\cite{Kiriansky:2002:SEV,Erlingsson:2006,Yee:2009}, and
    profiling~\cite{Harris:2005,Srivastava:1994}, in order to improve their
    performance and reliability.
\end{itemize}



